\section{Methods}
\label{sec:methods}
% finished

% \subsection{The Challenge Database}
% \label{subsec:database}
% % finished

% The Challenge database \cite{Oliveira_2021_CirCor} provides multiple PCG recordings collected at different auscultation locations for a number of young subjects. The annotations of the existence of heart murmurs are provided for each recording, along with annotations for heart sound segmentation annotation. The clinical outcomes are provided for each subject. For algorithm development, we divided the public subset of the Challenge database into the training set and the validation set with a ratio of 8:2. This split was stratified on the categorical attributes ``Age'', ``Sex'', ``Pregnancy status'' and the prediction targets ``Murmur'', ``Outcome''.


\subsection{Preprocess Pipeline}
\label{subsec:preproc}
% finished

After a careful study of spectral characteristics of heart murmurs from medical literature \cite{Noponen_2007}, and with reference to previous work \cite{Schmidt_2010}, we constructed the PCG signal preprocessing pipeline as follows:
\begin{itemize}
    \item Resampling to 1000 Hz;
    \item Butterworth bandpass filtering of order 3 and cutoff frequencies 25 - 400 Hz;
    \item Z-score normalization to zero mean and unit variance.
\end{itemize}

\subsection{Neural Network Backbones}
\label{subsec:backbone}
% finished

Inspired by the work of wav2vec2 \cite{baevski2020wav2vec}, and under the consideration of exploring and utilizing the powerfulness of pretraining models on larger databases, we adopted a shrunken \texttt{wav2vec2} as one of our neural network backbones. We used the time-domain signals, namely the PCG waveforms, as model input, rather than the derived time-frequency-domain signals, for example, the spectrograms. Since PCGs have significantly lower sampling rates than conventional human voice audio signals, we reduced the dimension (number of channels) of the `wav2vec2` model's encoder and its depth (number of hidden layers).

% The `wav2vec2` model has a small block of convolutions in the bottom used as the feature extractor which extracts feature maps of a fixed dimension from the waveforms and feeds into ...

\input{tables/nn_backbones.tex}

Considering that PCGs share a similar physiological origin as electrocardiograms (ECGs), we further adjusted and tested several neural network backbones that have proven effective in ECG problems, including multi-branch convolutional neural networks (MultiBranch), SE-ResNet, TResNetS, TResNetF \cite{Kang_2022_cinc2021_iop}, and ResNet-NC \cite{ribeiro2020automatic} etc. We enlarged the kernel sizes of each convolution in these backbones by a factor of 2, which is the ratio of the sampling rates.

The efficacy of most of the neural network backbones is validated via experiments as illustrated in Figure \ref{fig:compare_nn}. The learning process of the wav2vec2 model was interrupted at an early stage. The cause for this abnormal phenomenon is left for further studies.

\input{plot_figures/compare_nn.tex}

\subsection{Multi-Task Learning}
\label{subsec:mtl}
% finished

The 2 Challenge tasks \cite{cinc2022} are per-patient classification tasks. It should be noted that the Challenge database \cite{Oliveira_2021_CirCor} provides per-recording annotations for the murmur detection task and heart sound segmentation annotations as well. We applied the multi-task learning (MTL) paradigm \cite{Caruana_1997_mtl} on each recording via hard parameter sharing. More precisely, we use one neural network model for all the tasks. Each task has its specific model head, typically a stack of linear layers concatenated to the shared backbone as discussed in Section \ref{subsec:backbone}. Our MTL paradigm is illustrated in Figure \ref{fig:mtl_paradigm}.

\input{plot_figures/mtl_paradigm.tex}

As depicted in Figure \ref{fig:mtl_comparison}, experiments showed that models (with the same backbone) using an additional segmentation head (denoted as ``MTL3'') usually outperformed models with only two classification heads (denoted as ``MTL2'') for the Challenge tasks.

\input{plot_figures/mtl_comparison.tex}

Our neural network models produce per-recording predictions for the Challenge tasks. To obtain per-patient predictions, we used the simple greedy rule described in Algorithm \ref{alg:greedy}.

\begin{algorithm}
% \SetKwComment{Comment}{/* }{ */}
\SetInd{0.2em}{2em}
\uIf{at least on recording positive}{
    \textbf{Positive} for the patient\;
}
\uElseIf{all recording negative}{
    \textbf{Negative} for the patient\;
}
\Else(\tcp*[h]{for murmur detection only}){
    \textbf{Unknown} for the patient\;
}
\caption{The algorithm to obtain per-patient predictions}\label{alg:greedy}
\end{algorithm}

\subsection{Training Setups}
\label{subsec:training}
% finished

For algorithm development, we divided the publicly available part of the Challenge database into the training set and the validation set with a ratio of 8:2. This split was stratified on the attributes ``Age'', ``Sex'', ``Pregnancy status'' and the prediction targets ``Murmur'', ``Outcome''.

The batch size was set at 32 for model training, with the maximum number of epochs set at 60. Model parameters were optimized using the AMSGrad variant of the AdamW optimizer \cite{adamw_amsgrad} along with the \texttt{OneCycle} scheduler \cite{smith2019one_cycle}. We froze the backbone from a specific epoch (usually 30), only updating the parameters of the task heads.

To alleviate overfitting on the training set, an early stopping callback was added. To further improve model transferability, we applied several types of augmentations to the batched training data stochastically:
\begin{itemize}
    \item adding coloured noises;
    \item polarity inversion (flipping).
\end{itemize}

We experimented with two types of loss functions: the asymmetric loss (denoted ``Loss-A''); the weighted binary cross entropy (denoted ``Loss-B''). The weights were obtained from the weight matrix of the Challenge scoring functions \cite{cinc2022}. The superiority of Loss-B was observed, as was illustrated in Figure \ref{fig:clf-se-resnet-lossA-vs-lossB}.

\input{plot_figures/loss.tex}

\subsection{Demographic Features}
\label{subsec:demo_feat}
% finished

For the public data of the Challenge, some demographic features are strongly correlated with the prediction target ``Outcome'', as can be inferred from Figure \ref{fig:outcome_corr}. Experiments and official phase submissions showed that an auxiliary random forest classifier using these features and the murmur predictions improved the outcome scores (reduced the outcome cost). However, we did not use such auxiliary models in our final submission, since the distribution of these features might be completely different in the hidden data. Moreover, no supportive medical literature was found to support this point.

\input{plot_figures/dem_features.tex}
